from __future__ import print_function

import tensorflow as tf

import os
from tensorflow import math

from my_model import cnn
import numpy as np
import data_loader


os.environ["CUDA_VISIBLE_DEVICES"] = "1"

# Training settings
batch_size = 32
epochs = 200
lr = 0.01
momentum = 0.9
no_cuda = False
seed = 8
log_interval = 10
l2_decay = 5e-4
root_path = "./dataset/"
source_name = "amazon"
target_name = "webcam"
num_batches = 20


cuda = not no_cuda and tf.test.is_gpu_available()

tf.random.set_random_seed(seed)
if cuda:
    tf.device(seed)  # tf.matmul(seed)

kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}

#tf.data.Dataset.from_tensor_slices((target_train_loader,    target_test_loader)).batch(batch_size).repeat()
#tf.data.dataset(load_training)
#
# source_loader = data_loader.load_training(root_path, source_name, batch_size, kwargs)
# target_train_loader = data_loader.load_training(root_path, target_name, batch_size, kwargs)
# target_test_loader = data_loader.load_testing(root_path, target_name, kwargs)
#
# len_source_dataset = len(source_loader.dataset)
# len_target_dataset = len(target_test_loader.dataset)
# len_source_loader = len(source_loader)
# len_target_loader = len(target_train_loader)

mnist = tf.contrib.learn.datasets.load_dataset("mnist")
  train_data = mnist.train.images  # Returns np.array
  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
  test_data = mnist.test.images  # Returns np.array
  test_labels = np.asarray(mnist.test.labels, dtype=np.int32)




def load_pretrain(model):


    tf.reset_default_graph()
    # Add ops to save and restore all the variables.
    saver = tf.train.Saver()

    # Later, launch the model, use the saver to restore variables from disk, and
    # do some work with the model.
    with tf.Session() as sess:
        # Restore variables from disk.
        saver.restore(sess, "/tmp/model.ckpt")
        print("Model restored.")
        # Check the values of the variables
        # print("v1 : %s" % v1.eval())
        # print("v2 : %s" % v2.eval())
        # saver = tf.train.import_meta_graph('my_test_model-1000.meta')
        # https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
        # have to add my trained model
        # with tf.Session() as sess:
        #     url = tf.train.import_meta_graph('my_test_model-1000.meta')
        #     pretrained_dict= url.restore(sess, tf.train.latest_checkpoint('./'))
        #     model_dict = model.state_dict()
        # for k, v in model_dict.items():
        #     if not "cls_fc" in k:
        #         model_dict[k] = pretrained_dict[k[k.find(".") + 1:]]
        # model.load_state_dict(model_dict)
        # return model

def train(epoch, model):
    # LEARNING_RATE = lr / tf.math((1 + 10 * (epoch - 1) / epochs), 0.75)
    # print('learning rate{: .4f}'.format(LEARNING_RATE))


    optimizer = tf.train.AdamOptimizer(0.01,beta1=0.9, beta2=0.9, epsilon=l2_decay)

    iter_source = iter(source_loader)
    iter_target = iter(target_train_loader)
    num_iter = len_source_loader
    for i in range(1, num_iter):
        data_source, label_source = iter_source
        data_target, _ = iter_target
        if i % len_target_loader == 0:
            iter_target = iter(target_train_loader)
        if cuda:
            data_source, label_source = data_source.cuda(), label_source.cuda()  # tf.matmul(data_source)
            data_target = data_target.cuda()  # tf.matmul(data_target)
        data_source, label_source = tf.Variable(data_source), tf.Variable(label_source)
        data_target = tf.Variable(data_target)

        label_source_pred, loss_mmd = model(data_source, data_target)
        loss_cls = tf.losses.sparse_softmax_cross_entropy(label_source_pred, label_source)
        gamma = 2 / (1 + math.exp(-10 * (epoch) / epochs)) - 1
        loss = loss_cls + gamma * loss_mmd

        if i % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\tsoft_Loss: {:.6f}\tmmd_Loss: {:.6f}'.format(
                epoch, i * len(data_source), len_source_dataset,
                       100.00 * i / len_source_loader, loss.data[0], loss_cls.data[0], loss_mmd.data[0]))
        train_op = optimizer.minimize(loss)

        with tf.Session() as sess:
           for i in range(num_batches):
                _, loss_val = sess.run([train_op, loss])

    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={"x": train_data},
        y=train_labels,
        batch_size=100,
        num_epochs=None,
        shuffle=True)
    mnist_classifier.train(
        input_fn=train_input_fn,
        steps=20000,
        hooks=[logging_hook])


def test(model):
    tf.estimator.Estimator(model)  # is it correct
    test_loss = 0
    correct = 0

    for data, target in target_test_loader:
        if cuda:
            data, target = tf.matmul(data), tf.matmul(target)  # data.device
        data, target = tf.Variable(data ), tf.Variable(target)
        s_output, t_output = model(data, data)
        test_loss +=tf.losses.sparse_softmax_cross_entropy(data,target)
      #  pred= tf.nn.sparse_softmax_cross_entropy_with_logits(s_output), target, size_average=False).data[
       # 0]  # sum up batch loss

        pred = tf.argmax(s_output, 1) # get the index of the max log-probability

        correct+= pred.eval(feed_dict={data, target})


    test_loss /= len_target_dataset
    print('\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format(
        target_name, test_loss, correct, len_target_dataset,
        100. * correct / len_target_dataset))
    return correct


if __name__ == '__main__':
    model = cnn(num_classes=31)
    correct = 0
    print(model)
    if cuda:
        model.device()  # tf.matmul(model)
    model = load_pretrain(model)
    for epoch in range(1, epochs + 1):
        train(epoch, model)
        t_correct = test(model)
        if t_correct > correct:
            correct = t_correct
        print('source: {} to target: {} max correct: {} max accuracy{: .2f}%\n'.format(
            source_name, target_name, correct, 100. * correct / len_target_dataset))
